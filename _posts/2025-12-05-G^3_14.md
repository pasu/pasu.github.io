---
layout: post
title: G³周报(14)
\mathrm{d}Ate: 2025-12-05 10:27:00
tags: G³
categories: G^3
giscus_comments: true
related_posts: true
pretty_table: true
---

## **GSWT：基于王氏瓷砖的高斯泼溅技术**

* **解决大规模地形生成难题**：针对3D高斯泼溅（3DGS）难以从单一像本合成大规模或无限地形的问题，提出了一种基于“王氏瓷砖”（Wang Tiles）的创新框架。
* **实现无缝拼接与多样性**：该方法通过在每个瓷砖中编码带有边界约束的局部高斯场，实现了高斯场的随机且连续平铺，从而能够程序化地生成具有高空间多样性的广阔地形。
* **优化的实时渲染**：引入了专门针对3DGS王氏瓷砖特性的渲染优化方案（如预排序和平铺策略），成功实现了大规模3DGS地形的实时渲染。
* **完整的构建流程**：文章展示了从多层次细节（LOD）的样本重建，到利用语义感知图割算法生成瓷砖，再到最终运行时高效合成与渲染的完整技术管线。
* [https://yunfan.zone/gswt_webpage/](https://yunfan.zone/gswt_webpage/)

## **解析游戏中的模糊技术：从基础算法到“Dual Kawase”模糊**

* **模糊技术在游戏中的核心作用**：文章解释了模糊效果（Blur）是现代电子游戏后期处理（Post-processing）和用户界面（GUI）设计中的基础。它被广泛用于实现景深（Depth of Field）、光晕（Bloom）以及磨砂玻璃风格的UI背景等效果。
* **实时模糊的演进与挑战**：虽然“模糊”概念简单（即取周围像素的平均值），但在游戏中实现高效的实时模糊需要平衡数学理论与硬件性能。文章回顾了从简单的方框模糊（Box Blur）到高斯模糊（Gaussian Blur）的演变过程。
* **Dual Kawase模糊的优势**：文章重点推介了“Dual Kawase”模糊算法。这是一种经过优化的技术，通过下采样（downsampling）和上采样（upsampling）的交替迭代，能在保持极高性能的同时，生成高质量、平滑且美观的模糊效果，非常适合实时渲染场景。
* **交互式学习体验**：该网页提供了一个图形编程的交互式旅程，利用WebGL技术展示了不同模糊算法的实际运行效果。读者无需编程知识，即可通过调整参数实时观察光晕、动态模糊等效果的变化。
* **技术细节与性能权衡**：文章深入探讨了频率空间、低通滤波器以及双线性插值等图形学概念，分析了不同算法在GPU上的性能表现，解释了图形程序员通过“Dual Kawase”算法如何解决速度与画质的矛盾。
* [https://blog.frost.kiwi/dual-kawase/](https://blog.frost.kiwi/dual-kawase/)

## **全局光照技术：多重反弹间接光渲染**

* 单次反弹的间接光照无法完全捕捉复杂的光线交互（如色彩溢出），因此实现多重反弹对于提升场景的真实感至关重要。
* 文章介绍了一种基于屏幕空间探针网格（probe grid）的迭代方法。该方法将上一次反弹计算出的间接光照结果存储在探针中，并将其作为下一次反弹计算的新光源。
* 通过重复利用光线追踪和探针采样流程，系统可以迭代计算第二次、第三次乃至更多次的反弹，并将每次反弹的结果累加起来，形成最终的全局光照效果。
* 这种方法有效地复用了现有GI框架，虽然每次额外反弹都会增加性能开销，但通常2-3次反弹就能在视觉质量和实时性能之间取得良好平衡。
* [https://gpuopen.com/learn/gi-1-2-multibounce-indirect-rendering/](https://gpuopen.com/learn/gi-1-2-multibounce-indirect-rendering/)

## **CityEngine 2025.1 新功能概览**

* **CGA 几何修改功能 (Geometry Modifications)**
    引入了全新的 `modify` 操作和 `recompose` 策略，允许用户直接在 CGA 规则中对几何体部件进行变换、分割及重新连接。这突破了以往正交形态的限制，支持创建复杂的屋顶和当代建筑设计。
* **Visual CGA 无代码立面设计**
    `ESRI.lib` 中新增了立面组件，设计人员无需编写代码即可将体量模型转化为逼真的建筑立面。Visual CGA 编辑器还增加了参数连接功能（如共享楼层数）和内置文档，大大简化了过程化建模流程。
* **街道设计器 (Street Designer) 增强**
    改进了车道与人行道的区分显示，并新增了对象属性以展示车道用途（如公交、自行车）及相对位置。此外，针对 OpenStreetMap (OSM) 导入的数据进行了优化，并提升了街道编辑的性能和几何稳定性。
* **引入 Python 3 API (Beta)**
    推出了 Python 3 API 测试版，将 Python 集成从封闭的自动化工具转变为开放平台。用户现在可以集成 ArcPy、ArcGIS API for Python 以及数千个第三方库，用于开发定制解决方案和应用。
* **教程与文档更新**
    扩展并更新了教程目录，新增了关于 CGA 过程化建模和数据导入的系列教程，并对 Python 脚本、地形建模等现有教程进行了翻新和视觉优化。
* [https://doc.arcgis.com/en/cityengine/latest/whats-new/cityengine-whats-new.htm](https://doc.arcgis.com/en/cityengine/latest/whats-new/cityengine-whats-new.htm)

## **在Quest 2上实现Lumen与GPU烘焙的融合**

* **Lumen与GPU烘焙混合工作流**：为了解决Quest 2上多人VR游戏《Mannequin》光照设计的迭代速度和性能问题，团队开发了一套创新流程。利用Lumen的快速实时预览功能进行开发，最终通过GPU Lightmass进行静态烘焙，同时保持两者视觉效果的高度一致。
* **针对硬件的定制化优化**：针对Quest 2的性能限制，采用了多项优化措施，包括使用“完全粗糙”（Fully Rough）材质并通过自定义CubeMap模拟反射，以及利用几何脚本（Geometry Script）生成的网格来模拟体积光束（Light Shafts），避免了昂贵的全局体积雾计算。
* **低成本的伪Bloom效果**：由于移动端HDR开销过大，团队放弃了传统的Bloom后处理，转而使用“GlowingQuad”插件，通过基于视角的面片折叠技术，以极低的性能成本实现了令人信服的辉光效果。
* **跨平台光照方案**：利用“光照场景”（Lighting Scenarios）技术，实现了同一套关卡内容适配不同硬件。Quest版本使用全静态光照，而PC版本则通过调整灯光移动性（Movable），在保持基础烘焙效果的同时增加了动态阴影。
* [https://real-mrbeam.github.io/2025/10/28/Lighting-Mannequin.html](https://real-mrbeam.github.io/2025/10/28/Lighting-Mannequin.html)

## **Segment-Geospatial：利用 SAM 模型进行地理空间数据分割的 Python 包**

* **地理空间数据分析利器**：这是一个专门为处理地理空间数据（GeoTIFF 等）而设计的 Python 包，利用 Meta 的 Segment Anything Model (SAM) 实现高效的图像分割，旨在简化地理空间分析流程。
* **多样化的分割功能**：支持自动生成对象掩膜、通过交互式输入（点、边界框）分割遥感图像、利用文本提示（Text Prompts）进行分割，以及处理时间序列遥感影像。
* **完整的工作流支持**：能够从 TMS 服务器下载地图瓦片，将分割结果保存为多种常见的矢量格式（如 GeoPackage, Shapefile, GeoJSON），并提供在交互式地图上可视化结果的功能。
* **灵活的安装与扩展**：支持 PyPI 和 Conda 安装，提供多种可选依赖项（如 Fast SAM, HQ-SAM, Grounding DINO），用户可根据需求定制安装内容，同时支持在 GPU（推荐）和 CPU 环境下运行。
* **广泛的集成与应用**：可与 QGIS 和 ArcGIS 等桌面 GIS 软件结合使用，支持在 Google Colab 和 AWS 等云平台上运行，适用于学术研究和实际地理信息项目。
* [https://github.com/opengeos/segment-geospatial](https://github.com/opengeos/segment-geospatial)

## **AI 图像与视频生成的底层原理（3Blue1Brown 客座视频 / Welch Labs）**

* **CLIP 模型与语义理解**：视频首先解释了 CLIP（对比语言-图像预训练）如何将文本和图像映射到同一个高维向量空间中。这使得 AI 能够通过“向量算术”理解概念（例如，从“戴帽子”的向量中减去“帽子”的向量），从而连接语言与视觉。
* **扩散模型的核心机制**：详细阐述了扩散模型（Diffusion Models）的工作原理。模型并非简单地一步步去除噪声，而是学习了一个**向量场**（Vector Field），该场预测了从噪点状态回到原始清晰图像的方向（即预测总噪声）。
* **解决“模糊平均”问题**：通过二维螺旋线的直观类比，展示了如果仅进行确定性去噪，结果往往会坍缩成所有可能图像的“模糊平均值”。视频介绍了 **DDPM**（引入随机噪声步骤）和 **DDIM**（基于常微分方程的确定性方法）如何防止这种模糊，生成清晰锐利的图像。
* **无分类器引导（CFG）**：解释了如何让 AI 听懂并严格执行 Prompt。通过计算“有提示词”和“无提示词”两个向量场的差值并加以放大，强制模型偏离通用的生成路径，转向用户指定的具体概念。
* **视频生成的扩展**：最后以开源模型 **WAN 2.1** 为例，展示了视频生成本质上是将上述过程扩展到三维噪声块（时间 + 高度 + 宽度），从而生成连贯的动态画面。
* [https://www.youtube.com/watch?v=iv-5mZ_9CPY](https://www.youtube.com/watch?v=iv-5mZ_9CPY)